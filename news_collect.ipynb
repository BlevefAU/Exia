{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1558084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import mysql.connector\n",
    "import re  \n",
    "import json\n",
    "from datetime import datetime  \n",
    "\n",
    "from cnsenti import Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd66b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据库连接\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",  # MySQL服务器地址\n",
    "    user=\"root\",   # 用户名\n",
    "    password=\"\",  # 密码\n",
    "    database=\"soybean\"  # 数据库名称\n",
    ")\n",
    "\n",
    "# 创建游标对象，用于执行SQL查询\n",
    "cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d798d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt client set up\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=\"sk-X9zKww2VlHkICfI3Bz1suRX3lTlgm5TxfFehabn1oI2HA8wj\",\n",
    "    base_url=\"https://api.chatanywhere.tech/v1\"\n",
    ")\n",
    "\n",
    "\n",
    "def gpt_35_api_stream(messages: list):\n",
    "    \"\"\"为提供的对话消息创建新的回答 (流式传输)\n",
    "\n",
    "    Args:\n",
    "        messages (list): 完整的对话消息\n",
    "    \"\"\"\n",
    "    stream = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "    str = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "#             print(chunk.choices[0].delta.content, end=\"\")\n",
    "            str += chunk.choices[0].delta.content\n",
    "    return str\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    " \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b46d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# msg = \"【帮我判断下面这段材料，对于豆粕价格是上涨/下跌/趋稳的概率，只给我一个0到1的数字，0表示偏向下跌，0.5表示趋稳，1表示偏向上涨。】\"\n",
    "# msg += \"作者：东海期货 刘兵 近日，针对加拿大政府近日宣布对自华进口电动汽车、钢铝产品采取加征关税等限制措施，中方将对加拿大采取的相关限制措施发起“反歧视调查”，后续根据实际情况对加采取相应措施。目前中方对自加拿大进口油菜籽(5208, 89.00, 1.74%)等发起反倾销调查。 回顾近年来中加关系发展 2018年12月，孟晚舟在加拿大被扣押以来，中加关系持续紧张，并直接影响了双边的贸易往来，特别是在菜籽进口方面。2019年3月，由于政治紧张，且中国海关多次在加拿大进口的油菜籽中检出危险性有害物质，导致中国海关作出暂停进口的决定，并取消了部分加拿大油菜籽出口公司的出口许可证。此后，中国开始对加拿大菜籽实行相应进口限制，提高了对菜籽中杂质的限制要求。2021年9月，孟晚舟获释放，期间加中关系一直处于严重冻结状态，此后双边关系也几乎没有起色。2022年5月，中国取消了三年前对加拿大油菜籽的进口限制。2023年5月中加爆发外交事件，互相驱逐外交官，关系再度紧张。直至今年7月，加拿大外长乔利久违地访华，中加关系才初步改善。 近年来中加关系恶化对菜系进口的影响 2019年中国进口菜籽数量骤降42%至274万吨，其中对加拿大菜籽进口占比降至86%。2020年中国进口从俄罗斯补充进口增加75%约14.25万吨菜籽，全年中国菜籽进口同比增加了13%至311万，对加拿大菜籽进口依赖减至74%。2021年俄罗斯菜籽面积大幅扩增，但因干旱影响且提高了菜籽出口关税，导致中国直接进口菜籽数量继续下滑至265万吨。2022年俄乌战争导致港口封锁，中国进口俄菜籽停滞，同年 5月中国取消了对加拿大油菜籽的进口限制，全年中国直接进口菜籽数量再次下滑至196万。自2022年底至2023年上半年，中国进口加拿大菜籽出现爆发时增长， 2023年5月，中加关系再度紧张，同时因2022年加拿大菜籽减产导致2023年供应减少，菜籽进口受阶段性影响增加，但对2023/24年菜籽继续维持大规模采购。据统计，2023年中国全年采购菜籽同比增加180%至549万，其中加拿大菜籽进口占比92%，俄罗斯菜籽进口占比7%。 2024年以来，国内一季度进口菜籽利润丰厚，菜籽全年买船大幅增加。截至今年7月统计，中国累计进口菜籽283万吨，其中93.6%来自加拿大，5.5%来自俄罗斯。2023/24作物年度内，我们预计中国将进口菜籽525万吨，同比减少1.68%；预计中国直接进口菜油(8340, 2.00, 0.02%)或同比相当，约200万吨；直接进口菜籽粕280万，同比增加38.6%或77吨。2022年以来，中国对加拿大菜油直接进口量大幅减少，对俄罗斯直接进口的非转菜油量大幅增加。今年至7月统计，中国进口的菜油中58%来自俄罗斯，15.4%来自阿联酋，尽管对加拿大菜油进口微乎其微，但中国菜粕(2479, 38.00, 1.56%)的进口对加拿大的依赖程度却稳定在75%左右，另外将近18%菜粕进口来源于阿联酋。 中加关系再恶化对国内菜系供应可能的影响 加拿大菜籽进口以大厂浸出法压榨为主，出油率一般在42%，出粕率为56%左右。俄罗斯进口非转菜籽以作坊式压榨法为主，产浓香菜油，参考国内非转菜籽出油率约36%。总体看，2023/24年度中国进口菜籽压榨将产出菜粕275万吨菜粕（不含菜籽饼），217万吨菜油。结合国内直接进口供应量，2023/24年度直接进口且含进口菜籽压榨生产的菜粕量为555万， 进口直接且含进口菜籽压榨生产的菜油417万。 2019年-2021年，国内年平均进口加拿大菜籽为237万吨，比预期的2024年进口少了288万吨，大体折国内菜粕160万菜粕和120万吨菜油；2019-2021年年国内平均进口加拿大菜油95万吨、菜粕150万吨。 假设2024/25作物年度内，中国临时或反倾销调查结束后通过增加关税等措施限制菜籽进口，我们预计对进口影响不会超过2019-2021年平均水平。假设严重至此，菜油方面，国内至少可以通过恢复对加拿大的直接进口，以及增加俄罗斯进口来填补100万吨的缺口；菜粕方面，目前直接从加拿大直接进口量已是历史最高水平，来自阿联酋菜粕的进口供应同水平同2019-2021年大体相当。若如此，预期的菜粕缺口要远大于菜油。总体来看，若事态发展严重到2019-2021年水平，因限制菜籽进口而导致的菜油同2023/24年度内的缺口大体不会超过5%，但菜粕同2023/24年度供应的缺口预计将达到28%。如此情况下，叠加国产菜籽大榨作为补充供应，菜粕预期可供应量将350万吨左右，基本可以满足国内菜粕的刚需以及菜油缺口。如果菜粕缺口消费完全被豆粕(3111, 18.00, 0.58%)替代，预期全年将增加200万大豆(4521, -21.00, -0.46%)压榨，届时产出的豆油(7630, 36.00, 0.47%)将会是油脂额外增量。 中加关系恶化对油脂油料未来行情的影响 今年国内菜系整体供应充足，若事态严发，短期国内菜系供应依然可以维持稳定，中长期看菜粕菜粕刚需完全也可以满足，且可以通过豆粕维持蛋白供应稳定，菜油直接进口替代空间很大，且相关油脂可替代补充量也完全充足。此外，今年全球油料丰产既定，2024/25年度丰产交易趋向也未变，预计油脂油料及蛋白整体价格或不会因为菜系局部风险而出现单边行情。 短期，菜系风险溢价主导的菜棕价差、菜豆粕价差或会出现低估值修复行情。中期，油脂因油料丰产，以棕榈(7878, 112.00, 1.44%)油决定的油脂价格重心的趋势不变；国内豆粕供应预计十分充足，依然是决定蛋白价格重心的核心。远期，重点关注新季度天气可能引发的油料油脂较预期减产的风险，若出现减产交易趋向，菜系或相比豆系及棕榈油更具有弹性空间。\"\n",
    "# msg_text = \"【帮我判断下面这段材料，对于豆粕价格是上涨/下跌/趋稳的概率，如果无法判断就说无法判断，只给一句话不超过20字。】\"\n",
    "# msg_text += \"读这个链接：https://finance.eastmoney.com/a/202409053176593116.html\"\n",
    "\n",
    "# # messages = [{'role': 'user','content': msg},]\n",
    "\n",
    "# # call def gpt\n",
    "# # gpt_value = gpt_35_api_stream(messages)\n",
    "# # print(gpt_value)\n",
    "\n",
    "# messages = [{'role': 'user','content': msg_text},]\n",
    "# gpt = gpt_35_api_stream(messages)\n",
    "# print(gpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a680d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit the home page for M (Sina)\n",
    "res = requests.get('https://finance.sina.com.cn/money/future/M/')\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f99a8022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[豆粕]大宗商品到\"变盘时刻\"？高盛最新观点来了！(09-09 16:52)\n"
     ]
    }
   ],
   "source": [
    "link_list = []\n",
    "title_list = []\n",
    "context_list = []\n",
    "date_list = []\n",
    "i = 0\n",
    "\n",
    "li_tags = soup.find_all('li')\n",
    "\n",
    "for li_tag in li_tags:\n",
    "    # pass the top 2 news (fake news)\n",
    "    i +=1\n",
    "    if i<=2:\n",
    "        continue\n",
    "        \n",
    "    # fetch link and title\n",
    "    link = li_tag.find('a')['href']\n",
    "    title = li_tag.text\n",
    "    \n",
    "    # check the date of report\n",
    "    date_pattern = re.compile(r'(\\d{2}-\\d{2})')  \n",
    "    current_year = datetime.now().year  \n",
    "    \n",
    "    matches = date_pattern.findall(title).pop()\n",
    "    full_date = f\"{current_year}-{matches}\"  \n",
    "    \n",
    "    # 将字符串转换成日期对象  \n",
    "    date_obj = datetime.strptime(full_date, '%Y-%m-%d')  \n",
    "    today_obj = datetime.now().date()  \n",
    "    is_same_dayss = (date_obj.date() == today_obj)  \n",
    "    \n",
    "    # only today news\n",
    "    if is_same_dayss==False:\n",
    "        continue\n",
    "#     if i>20:\n",
    "#         continue\n",
    "    \n",
    "    # get the article text\n",
    "    text = \"\"\n",
    "    link_res = requests.get(link)\n",
    "    link_res.encoding = link_res.apparent_encoding\n",
    "    link_soup = BeautifulSoup(link_res.text, 'html.parser')\n",
    "    try:\n",
    "        text = link_soup.find(class_=\"article\").get_text()\n",
    "    except:\n",
    "        print(title)\n",
    "    \n",
    "    # append to list\n",
    "    link_list.append(link)\n",
    "    title_list.append(title)\n",
    "    context_list.append(text)\n",
    "    date_list.append(full_date)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "093fbdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "豆粕价格下跌的概率较大。\n",
      "豆粕价格趋稳的概率较高。\n",
      "豆粕价格下跌的概率较大。\n",
      "豆粕价格可能上涨，降雨频繁可能影响产量。\n",
      "对豆粕价格趋势影响不确定，需独立判断。\n",
      "豆粕价格下跌的概率较大。\n",
      "豆粕价格可能下跌。\n",
      "鉴于生猪行情保持景气度，豆粕价格可能上升。\n",
      "豆粕价格有上涨的概率。\n",
      "根据上述材料，对于豆粕价格是趋稳的概率较高。\n",
      "豆粕价格有上涨的概率。\n",
      "豆粕价格可能下跌。\n",
      "豆粕价格趋稳可能性较大。\n",
      "豆粕价格趋稳的概率较大。\n",
      "豆粕价格有上涨的概率。\n",
      "豆粕价格有上涨可能。\n",
      "下跌概率较大。\n",
      "豆粕价格趋稳的概率较大。\n",
      "豆粕价格可能趋稳，受强降雨天气和高温影响。\n",
      "豆粕价格趋势上涨的概率较大。\n",
      "豆粕价格下跌的概率较大。\n",
      "无法判断豆粕价格走势。\n",
      "豆粕价格有上涨的概率。\n",
      "豆粕价格有上涨的概率。\n",
      "豆粕价格有下跌的概率。\n",
      "豆粕价格趋稳的概率较高。\n",
      "豆粕价格有上涨的概率。\n"
     ]
    }
   ],
   "source": [
    "msg_text = \"【帮我判断下面这段材料，对于豆粕价格是上涨/下跌/趋稳的概率，只给一句话不超过20字。】\"\n",
    "\n",
    "gpt_list = []\n",
    "gpt_value_list = []\n",
    "\n",
    "for n in range(0,len(link_list)):\n",
    "    # remove token less than 100\n",
    "    if len(context_list[n]) <= 20:\n",
    "        gpt_list.append(\"\")\n",
    "        continue\n",
    "    \n",
    "    #check if token more than 4000 remove the last\n",
    "    new_context = context_list[n]\n",
    "    if len(context_list[n]) > 3200:\n",
    "        new_context = context_list[n][:-(len(context_list[n])-3200)]\n",
    "    \n",
    "    # find gpt text\n",
    "    messages_text = [{'role': 'user','content': msg_text+new_context},]\n",
    "    gpt = gpt_35_api_stream(messages_text)\n",
    "    gpt_list.append(gpt)\n",
    "    print(gpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58cca95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\bleve\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.622 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5\n",
      "豆粕价格下跌的概率较大。\n",
      "0\n",
      "豆粕价格趋稳的概率较高。\n",
      "0.5\n",
      "豆粕价格下跌的概率较大。\n",
      "0\n",
      "豆粕价格可能上涨，降雨频繁可能影响产量。\n",
      "0.5\n",
      "对豆粕价格趋势影响不确定，需独立判断。\n",
      "1\n",
      "豆粕价格下跌的概率较大。\n",
      "0\n",
      "豆粕价格可能下跌。\n",
      "0\n",
      "鉴于生猪行情保持景气度，豆粕价格可能上升。\n",
      "0.5\n",
      "豆粕价格有上涨的概率。\n",
      "1\n",
      "根据上述材料，对于豆粕价格是趋稳的概率较高。\n",
      "0.5\n",
      "豆粕价格有上涨的概率。\n",
      "1\n",
      "豆粕价格可能下跌。\n",
      "0\n",
      "豆粕价格趋稳可能性较大。\n",
      "0.5\n",
      "豆粕价格趋稳的概率较大。\n",
      "0.5\n",
      "豆粕价格有上涨的概率。\n",
      "1\n",
      "豆粕价格有上涨可能。\n",
      "1\n",
      "下跌概率较大。\n",
      "0\n",
      "豆粕价格趋稳的概率较大。\n",
      "0.5\n",
      "豆粕价格可能趋稳，受强降雨天气和高温影响。\n",
      "0.5\n",
      "豆粕价格趋势上涨的概率较大。\n",
      "1\n",
      "豆粕价格下跌的概率较大。\n",
      "0\n",
      "无法判断豆粕价格走势。\n",
      "0\n",
      "豆粕价格有上涨的概率。\n",
      "1\n",
      "豆粕价格有上涨的概率。\n",
      "1\n",
      "豆粕价格有下跌的概率。\n",
      "0\n",
      "豆粕价格趋稳的概率较高。\n",
      "0.5\n",
      "豆粕价格有上涨的概率。\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# fetch the value of gpt text\n",
    "gpt_value_list = []\n",
    "\n",
    "for n in range(0,len(gpt_list)):\n",
    "    senti = Sentiment(pos='pos.txt',  \n",
    "                      neg='neg.txt', \n",
    "                      merge=True,  \n",
    "                      encoding='utf-8')\n",
    "\n",
    "    result = senti.sentiment_count(gpt_list[n])\n",
    "    if result['pos']>result['neg']:\n",
    "        gpt_value_list.append(1)\n",
    "    elif result['pos']<result['neg']:\n",
    "        gpt_value_list.append(0)\n",
    "    else:\n",
    "        gpt_value_list.append(0.5)\n",
    "    print(gpt_list[n])\n",
    "    print(gpt_value_list[n])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ecd74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert into database\n",
    "for n in range(0,len(title_list)):\n",
    "    sql = \"INSERT INTO news (title, link,news_date,context,gpt,gpt_value) VALUES (%s, %s, %s,%s, %s, %s)\"\n",
    "    values = (title_list[n], link_list[n],date_list[n],context_list[n],gpt_list[n],float(gpt_value_list[n]))\n",
    "\n",
    "    cursor.execute(sql, values)\n",
    "    \n",
    "# 提交更改到数据库\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eaa8676",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for eastmoney news\n",
    "page_index = 1\n",
    "page_size = 20\n",
    "\n",
    "url = 'https://np-listapi.eastmoney.com/comm/web/getNewsByScColumns?client=web&biz=web_news_sccol&scColumns=278&order=1&page_index='+str(page_index)+'&page_size='+str(page_size)+'&req_trace=1725761781811&fields=code,showTime,title,mediaName,summary,realSort,image,url,uniqueUrl,Np_dst&types=1,20&callback=jQuery18306420563401369495_1725761781759&_=1725761781812'\n",
    "\n",
    "res = requests.get(url)\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "# convert to json\n",
    "cleaned_res = re.sub(r'jQuery\\d+_\\d+\\(', '', res.text)  \n",
    "cleaned_res = cleaned_res.rstrip(');')  # 去除末尾的');'\n",
    "res_json = json.loads(cleaned_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c27c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch news\n",
    "link_list = []\n",
    "title_list = []\n",
    "context_list = []\n",
    "date_list = []\n",
    "\n",
    "link = res_json['data']['list'][0]['url']\n",
    "\n",
    "for item in res_json['data']['list']:\n",
    "    \n",
    "    # 将字符串转换成日期对象  \n",
    "    today_obj = str(datetime.now().date())\n",
    "    is_same_dayss = (item['showTime'].split(' ')[0] == today_obj)  \n",
    "    \n",
    "    # only today news\n",
    "    if is_same_dayss==False:\n",
    "        continue\n",
    "    \n",
    "    link = item['url']\n",
    "    title = item['title']\n",
    "    full_date = item['showTime'].split(' ')[0]\n",
    "\n",
    "    # get the article text\n",
    "    link_res = requests.get(link)\n",
    "    link_res.encoding = link_res.apparent_encoding\n",
    "    link_soup = BeautifulSoup(link_res.text, 'html.parser')\n",
    "    text = link_soup.find(class_=\"txtinfos\").get_text()\n",
    "    \n",
    "    # append to list\n",
    "    link_list.append(link)\n",
    "    title_list.append(title)\n",
    "    context_list.append(text)\n",
    "    date_list.append(full_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9de40e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_text = \"【帮我判断下面这段材料，对于豆粕价格是上涨/下跌/趋稳的概率，只给一句话不超过20字。】\"\n",
    "\n",
    "gpt_list = []\n",
    "\n",
    "for n in range(0,len(link_list)):\n",
    "    # remove token less than 100\n",
    "    if len(context_list[n]) <= 20:\n",
    "        gpt_list.append(\"\")\n",
    "        continue\n",
    "    \n",
    "    #check if token more than 4000 remove the last\n",
    "    new_context = context_list[n]\n",
    "    if len(context_list[n]) > 3200:\n",
    "        new_context = context_list[n][:-(len(context_list[n])-3200)]\n",
    "    \n",
    "    # find gpt text\n",
    "    messages_text = [{'role': 'user','content': msg_text+new_context},]\n",
    "    gpt = gpt_35_api_stream(messages_text)\n",
    "    gpt_list.append(gpt)\n",
    "    print(gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b87a720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the value of gpt text\n",
    "gpt_value_list = []\n",
    "\n",
    "for n in range(0,len(gpt_list)):\n",
    "    senti = Sentiment(pos='pos.txt',  \n",
    "                      neg='neg.txt', \n",
    "                      merge=True,  \n",
    "                      encoding='utf-8')\n",
    "\n",
    "    result = senti.sentiment_count(gpt_list[n])\n",
    "    if result['pos']>result['neg']:\n",
    "        gpt_value_list.append(1)\n",
    "    elif result['pos']<result['neg']:\n",
    "        gpt_value_list.append(0)\n",
    "    else:\n",
    "        gpt_value_list.append(0.5)\n",
    "#     print(gpt_list[n])\n",
    "#     print(gpt_value_list[n])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b88f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert into database\n",
    "for n in range(0,len(title_list)):\n",
    "    sql = \"INSERT INTO news (title, link,news_date,context,gpt,gpt_value) VALUES (%s, %s, %s,%s, %s, %s)\"\n",
    "    values = (title_list[n], link_list[n],date_list[n],context_list[n],gpt_list[n],float(gpt_value_list[n]))\n",
    "\n",
    "    cursor.execute(sql, values)\n",
    "    \n",
    "# 提交更改到数据库\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819d061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
